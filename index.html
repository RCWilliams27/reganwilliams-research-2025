<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Home | Spectral Graph Theory & Quantum Mechanics</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      background-color: #f8f9fa;
      color: #333;
    }
    header {
      background-color: #343a40;
      color: white;
      padding: 1em;
      text-align: center;
    }
    nav {
      background-color: #495057;
      display: flex;
      justify-content: center;
      padding: 0.5em 0;
    }
    nav a {
      color: white;
      margin: 0 1em;
      text-decoration: none;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    main {
      max-width: 900px;
      margin: auto;
      padding: 2em;
    }
    section, .box {
      margin-bottom: 2em;
    }
    .box {
      background-color: #ffffff;
      border: 1px solid #dee2e6;
      padding: 1.5em;
      border-radius: 8px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    h2, h3 {
      margin-top: 0;
      color: #222;
    }
    .formula {
      text-align: center;
      font-family: Arial, sans-serif;
      font-weight: bold;
      font-size: 1.1em;
      margin: 1em 0;
    }
    footer {
      background-color: #343a40;
      color: white;
      text-align: center;
      padding: 1em 0;
      margin-top: 3em;
    }
    .image-row {
      display: flex;
      gap: 1rem;
      justify-content: space-between;
      flex-wrap: wrap;
      margin-top: 1em;
    }
    .image-row figure {
      flex: 1 1 calc(33% - 1rem);
      margin: 0;
      background: white;
      border: 1px solid #ccc;
      border-radius: 6px;
      padding: 0.5rem;
      box-sizing: border-box;
      text-align: center;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
      transition: box-shadow 0.3s ease;
    }
    .image-row figure:hover {
      box-shadow: 0 4px 10px rgba(0,0,0,0.2);
    }
    .image-row img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 0 auto;
      border-radius: 4px;
    }
    .image-row figcaption {
      font-size: 0.85rem;
      color: #666;
      margin-top: 0.3rem;
      font-style: italic;
    }
    @media (max-width: 600px) {
      .image-row figure {
        flex: 1 1 100%;
      }
    }
    .notes a {
      color: #007acc;
      text-decoration: none;
      font-weight: bold;
    }
    .notes a:hover {
      text-decoration: underline;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
  <header>
    <h1>Spectral Graph Theory and Quantum Mechanics</h1>
  </header>

  <nav>
    <a href="index.html">Home</a>
    <a href="visualizations.html">Visualizations</a>
    <a href="results.html">Results</a>
    <a href="meet.html">Meet Us</a>
  </nav>

  <main>
    <section>
      <h2>Project Overview</h2>
      <p>This research project explores the intersection of spectral graph theory and quantum mechanics. We focus on computing various types of graph entropy to understand their significance in both mathematical and physical contexts. Our goal is to study entropy by directly calculating it as well as examining how graph operations like rewiring and gluing affect its behavior. This site presents theoretical concepts, visualizations, and daily notes from our research process.</p>
    </section>

    <section class="notes">
      <h2>Daily Notes</h2>
      <a href="Spectral-Graph-Theory-and-Quantum-Mechanics-Research-Notes.pdf" target="_blank">
        Spectral Graph Theory & Quantum Mechanics Research Notes
      </a>
    </section>

    <section>
      <h2>Entropy & Graphs</h2>

      <div class="box">
        <h3>What Is Entropy?</h3>
        <p>Entropy measures uncertainty or disorder in a system. More specifically, entropy reflects the number of different microscopic configurations (microstates) that correspond to the same observable condition (macrostate). Because there are naturally many more disordered configurations than ordered ones, systems evolve from low-entropy (more ordered) states to high-entropy (less ordered) states.</p>
        <div class="image-row">
          <figure>
            <img src="images/Entropy1.png" alt="Entropy illustration showing microstates" />
            <figcaption>Image: Kristina Armitage / Quanta Magazine</figcaption>
          </figure>
          <figure>
            <img src="images/Entropy2.png" alt="Entropy representation with particles" />
            <figcaption>Image: Kristina Armitage / Quanta Magazine</figcaption>
          </figure>
          <figure>
            <img src="images/Entropy3.png" alt="Entropy visualization example" />
            <figcaption>Image: Kristina Armitage / Quanta Magazine</figcaption>
          </figure>
        </div>
      </div>

      <div class="box">
        <h3>Why Is It Important?</h3>
        <p>Entropy appears across many fields of science, helping us understand everything from heat to information to the structure of the universe. There are several major types of entropy:</p>
        <ul style="list-style-type: disc; padding-left: 1.2em;">
          <li style="margin-bottom: 1.3em;"><strong>Boltzmann Entropy:</strong> Measures how many microscopic ways a system (like a gas) can be arranged while appearing the same overall. More arrangements = higher entropy.</li>
          <li style="margin-bottom: 1.3em;"><strong>Gibbs Entropy:</strong> A generalization of Boltzmann entropy that uses probabilities to account for how likely each arrangement is. It measures average disorder.</li>
          <li style="margin-bottom: 1.3em;"><strong>Shannon Entropy:</strong> Measures how uncertain or unpredictable information is; higher entropy means more randomness in a message.</li>
          <li style="margin-bottom: 1.3em;"><strong>Von Neumann Entropy:</strong> A quantum version of Shannon entropy that measures how uncertain or disordered a quantum system is.</li>
          <li style="margin-bottom: 1.3em;"><strong>Bekenstein-Hawking Entropy:</strong> Measures the hidden information inside a black hole.</li>
        </ul>
      </div>

      <div class="box">
        <h3>Why Graphs?</h3>
        <p>Many real-world systems (epidemics, social networks, the internet, physical processes) exhibit entropy. Measuring that entropy helps us understand and predict their behavior. But these systems are too complex to model with continuous equations alone. By discretizing them and applying linear algebra techniques, we can study them more effectively. Graph theory provides a natural framework for this discretization: we represent complex systems as networks of vertices and edges and analyze their structure using matrices.</p>
      </div>

      <div class="box">
        <details>
          <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em;">Types of Graphs</summary>
          <ul style="margin-top: 1em; line-height: 1.6em; padding-left: 1.2em;">
            <li>Path Graph</li>
            <li>Tree Graph</li>
            <li>Cycle Graph</li>
            <li>Star Graph</li>
            <li>Lattice</li>
            <li>Torus</li>
            <li>Complete Bipartite Graph</li>
            <li>Complete Graph</li>
            <a href="pdfs/Types-of-Graphs-Notes.pdf" target="_blank">Types of Graphs Research Notes</a>
          </ul>
        </details>
      </div>

      <h2>Von Neumann Entropy</h2>

      <div class="box">
        <h3>What Is Von Neumann Entropy?</h3>
        <p>Von Neumann entropy measures how complex or disordered a graph is by analyzing its structure.</p>
        <p><strong>How We Calculate It:</strong></p>
        <p><strong>Degree matrix (D):</strong> Shows how many connections each vertex has. Each diagonal entry gives the number of edges connected to that vertex.</p>
        <p><strong>Adjacency matrix (A):</strong> Shows which vertices are directly connected. If two vertices share an edge, the matrix entry is 1; otherwise, it's 0.</p>
        <p><strong>Laplacian matrix (∆):</strong> Formed by subtracting the adjacency matrix from the degree matrix. This matrix captures how the graph is structured overall.</p>
        <p class="formula">∆ = D − A</p>
        <p>We then analyze the eigenvalues (λ) of the Laplacian to compute von Neumann entropy using:</p>
        <p class="formula">S = −∑ λ log λ</p>
      </div>

      <div class="box">
        <details>
          <summary style="cursor: pointer; font-weight: bold; font-size: 1.1em;">Example: Von Neumann Entropy of the Cycle Graph C₄</summary>
          <div style="margin-top: 1em; line-height: 1.6em;">
             <p><strong>Step 1:</strong> Label vertices \(v_1\) through \(v_4\), connected in a cycle.</p>

              <svg width="200" height="200" viewBox="0 0 200 200">
              <line x1="50" y1="50" x2="150" y2="50" class="edge"/>
              <line x1="150" y1="50" x2="150" y2="150" class="edge"/>
              <line x1="150" y1="150" x2="50" y2="150" class="edge"/>
              <line x1="50" y1="150" x2="50" y2="50" class="edge"/>
  
              <circle cx="50" cy="50" r="10" class="node"/>
              <circle cx="150" cy="50" r="10" class="node"/>
              <circle cx="150" cy="150" r="10" class="node"/>
              <circle cx="50" cy="150" r="10" class="node"/>

              <text x="50" y="45" class="label">\(v_1\)</text>
              <text x="150" y="45" class="label">\(v_2\)</text>
              <text x="150" y="170" class="label">\(v_3\)</text>
              <text x="50" y="170" class="label">\(v_4\)</text>
              </svg>
            <p><strong>Step 2:</strong> Degree matrix \(D\):<br>
              \[
              D = \begin{bmatrix}
              2 & 0 & 0 & 0 \\
              0 & 2 & 0 & 0 \\
              0 & 0 & 2 & 0 \\
              0 & 0 & 0 & 2
              \end{bmatrix}
              \]
            </p>
            <p><strong>Step 3:</strong> Adjacency matrix \(A\):<br>
              \[
              A = \begin{bmatrix}
              0 & 1 & 0 & 1 \\
              1 & 0 & 1 & 0 \\
              0 & 1 & 0 & 1 \\
              1 & 0 & 1 & 0
              \end{bmatrix}
              \]
            </p>
            <p><strong>Step 4:</strong> Laplacian \(\Delta = D - A\):<br>
              \[
              \Delta = \begin{bmatrix}
              2 & -1 & 0 & -1 \\
              -1 & 2 & -1 & 0 \\
              0 & -1 & 2 & -1 \\
              -1 & 0 & -1 & 2
              \end{bmatrix}
              \]
            </p>
            <p><strong>Step 5:</strong> Eigenvalues: \( \{0, 2, 4, 2\} \)</p>
            <p><strong>Step 6:</strong> Compute \(S = \sum \lambda \log \lambda \), excluding \(0\):<br>
              \[
              S = 2 \log 2 + 2 \log 2 + 4 \log 4 ≈ 8.3178
              \]
            </p>
          </div>
        </details>
      </div>

      <div class="box">
        <h3>Key Takeaways</h3>
        <ul>
          <li>Entropy quantifies uncertainty or disorder in a system.</li>
          <li>Different kinds of entropy connect concepts from thermodynamics, quantum mechanics, and information theory.</li>
          <li>To make complex systems more manageable, we discretize them and use linear algebra to explore their behavior.</li>
          <li>Von Neumann entropy applies these principles to graphs, helping us measure and understand their structure and complexity.</li>
        </ul>
      </div>
    </section>
  </main>

  <footer>
    <p>Site built by Regan Williams.</p>
  </footer>
</body>
</html>
